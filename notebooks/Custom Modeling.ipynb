{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-venv-mlflow",
      "display_name": "Python (env mlflow)",
      "language": "python"
    },
    "analyzedDataset": "train",
    "creator": "tyfrec.test@yahoo.com",
    "createdOn": 1666967089230,
    "tags": [],
    "customFields": {},
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.8.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "tyler.freckmann@dataiku.com",
    "dkuGit": {
      "gitReference": {
        "remote": "https://github.com/tylerfreckmann/coder_handson.git",
        "checkout": "refs/heads/online_test",
        "remotePath": "notebooks/Custom Modeling.ipynb",
        "lastHash": "5add690d663d35db7c3d121618bc4180d5f56bf6",
        "lastTimestamp": 1667333734000,
        "isDirty": false
      },
      "lastInteraction": 1667333763729
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nimport pandas as pd\nimport mlflow\nimport warnings\n\nfrom datetime import datetime\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_validate, StratifiedKFold\nfrom dataikuapi.dss.ml import DSSPredictionMLTaskSettings\nwarnings.filterwarnings(\u0027ignore\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Replace these constants by your own values\nEXPERIMENT_TRACKING_FOLDER_NAME \u003d \"tracking\"\nEXPERIMENT_TRACKING_FOLDER_CONNECTION \u003d \"filesystem_folders\"\nEXPERIMENT_NAME \u003d \"custom-modeling\"\n\nMLFLOW_CODE_ENV_NAME \u003d \"mlflow\"\nSAVED_MODEL_NAME \u003d \"custom-model\"\nDATASET_TRAINING \u003d \"train\""
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Some utils\ndef now_str() -\u003e str:\n    return datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment tracking (scikit-learn)\n\nThis notebook contains a simple example to showcase the new Experiment Tracking capabilities of Dataiku. It explains how to perform several runs with different parameters, select the best run and promote it as a Saved Model version in a Dataiku Flow. It leverages:\n* the scikit-learn package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the training data\n\nOur training data lives in the `labeled` Dataset, let\u0027s load it in a pandas DataFrame and see what it looks like:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "client \u003d dataiku.api_client()\nproject \u003d client.get_default_project()\ntraining_dataset \u003d dataiku.Dataset(DATASET_TRAINING)\ndf \u003d training_dataset.get_dataframe()\ndf.head()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are working on a *binary classification* problem here, which is to predict whether or not a given customer is high value. This outcome is reflected by the `high_value` column which can either take the \"0.0\" or \"1.0\" values."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "target_name \u003d \"high_value\"\ntarget \u003d df[target_name]\ndata \u003d df.drop(columns\u003d[target_name])"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get-or-create Managed Folder (WIP)\nproject_folders \u003d project.list_managed_folders()\nfolder \u003d None\nif len(project_folders) \u003e 0:\n    for mf in project_folders:\n        if mf[\"name\"] \u003d\u003d EXPERIMENT_TRACKING_FOLDER_NAME:\n            folder_id \u003d mf[\"id\"]\n            print(f\"Found experiment tracking folder {EXPERIMENT_TRACKING_FOLDER_NAME} with id {mf[\u0027id\u0027]}\")\n            folder \u003d project.get_managed_folder(odb_id\u003dfolder_id)\n            break\n        else:\n            continue\n    # -- If you reach this point, you didn\u0027t find the experiment tracking folder among the existing ones.\n    if not folder:\n        print(\"Experiment tracking folder not found. Creating it...\")\n        folder \u003d project.create_managed_folder(EXPERIMENT_TRACKING_FOLDER_NAME,\n                                   connection_name\u003dEXPERIMENT_TRACKING_FOLDER_CONNECTION)\nelse:\n    print(\"No folder found in project. Creating one for experiment tracking...\")\n    # Write the creation of the mf code here.\n    folder \u003d project.create_managed_folder(EXPERIMENT_TRACKING_FOLDER_NAME,\n                                       connection_name\u003dEXPERIMENT_TRACKING_FOLDER_CONNECTION)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing the experiment\n\nTo prepare the grounds for our experiments, we need to create a few handles and define which MLFlow experiment we\u0027ll collect our runs into:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a mlflow_extension object to easily collect information for the promotion step\nmlflow_extension \u003d project.get_mlflow_extension()\n\n# Create a handle for the mlflow client\nmlflow_handle \u003d project.setup_mlflow(managed_folder\u003dfolder)\n\n# Set the experiment\nexperiment \u003d mlflow.set_experiment(experiment_name\u003dEXPERIMENT_NAME)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experimenting\n\nThe goal of experiment tracking is to *instrument the iterative process of ML model training* by collecting all parameters and results of each trial. To be more specific, within an **experiment**, you perform multiple **runs**, each run being different from the others because of the **parameters** you use for it. You also need to specific which **metrics** to track, they will reflect the performance of the model for a given set of parameters.\n\nIn this notebook example, if you want to produce experiment runs:\n* edit the parameters in the 3.1 cell and run it\n* run the 3.2 cell to effectively... perform the run ðŸ™‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the parameters of our run"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create run name\nrun_params \u003d {}\nrun_metrics \u003d {}\n\n# Define run parameters\n# -- Which columns to retain ?\ncategorical_cols \u003d [\"gender\", \"ip_country_code\"]\nrun_params[\"categorical_cols\"] \u003d categorical_cols\nnumerical_cols \u003d [\"age\", \"price_first_item_purchased\", \"pages_visited\", \"campaign\"]\nrun_params[\"numerical_cols\"] \u003d numerical_cols\n\n# --Which algorithm to use? Which hyperparameters for this algo to try?\n# ---Example: Gradient Boosting\nhparams \u003d {\"n_estimators\": 300,\n          \"loss\": \"exponential\",\n          \"learning_rate\": 0.1,\n          \"max_depth\": 3,\n          \"random_state\": 42}\nclf \u003d GradientBoostingClassifier(**hparams)\nmodel_algo \u003d type(clf).__name__\nrun_params[\"model_algo\"] \u003d model_algo\nfor hp in hparams.keys():\n    run_params[hp] \u003d hparams[hp]\n\n# --Which cross-validation settings to use?\nn_cv_folds \u003d 5\ncv \u003d StratifiedKFold(n_splits\u003dn_cv_folds)\nrun_params[\"n_cv_folds\"] \u003d n_cv_folds\nmetrics \u003d [\"f1_macro\", \"roc_auc\"]\n\n# --Let\u0027s print all of that to get a recap:\nprint(f\"Parameters to log:\\n {run_params}\")\nprint(100*\u0027-\u0027)\nprint(f\"Metrics to log:\\n {metrics}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performing the run and logging parameters, metrics and the model"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "run_ts \u003d now_str()\nrun_name \u003d f\"run-{run_ts}\"\nwith mlflow.start_run(run_name\u003drun_name) as run:\n    run_id \u003d run.info.run_id\n    print(f\"Starting run {run_name} (id: {run_id})...\")\n    # --Preprocessing\n    categorical_preprocessor \u003d OneHotEncoder(handle_unknown\u003d\"ignore\")\n    preprocessor \u003d ColumnTransformer([\n        (\u0027categorical\u0027, categorical_preprocessor, categorical_cols),\n        (\u0027numerical\u0027, \u0027passthrough\u0027, numerical_cols)])\n    \n    # --Pipeline definition (preprocessing + model)\n    pipeline \u003d make_pipeline(preprocessor, clf)\n    \n    # --Cross-validation\n    print(f\"Running cross-validation...\")\n    scores \u003d cross_validate(pipeline, data, target, cv\u003dcv, scoring\u003dmetrics)\n    for m in [f\"test_{mname}\" for mname in metrics]:\n        run_metrics[f\"mean_{m}\"] \u003d scores[m].mean()\n        run_metrics[f\"std_{m}\"] \u003d scores[m].std()\n        \n    # --Pipeline fit\n    pipeline.fit(X\u003ddata, y\u003dtarget)\n    # --Log the order of the class label\n    run_params[\"class_labels\"] \u003d [str(c) for c in pipeline.classes_.tolist()]\n    \n    # --Log parameters, metrics and model\n    mlflow.log_params(params\u003drun_params)\n    mlflow.log_metrics(metrics\u003drun_metrics)\n    artifact_path \u003d f\"{model_algo}-{run_id}\"\n    mlflow.sklearn.log_model(sk_model\u003dpipeline, artifact_path\u003dartifact_path)\n    \n    # --Set useful information to faciliate run promotion\n    mlflow_extension.set_run_inference_info(run_id\u003drun_id,\n                                            prediction_type\u003d\"BINARY_CLASSIFICATION\",\n                                            classes\u003drun_params[\"class_labels\"],\n                                            code_env_name\u003dMLFLOW_CODE_ENV_NAME,\n                                            target\u003d\"high_value\")\n    print(f\"DONE! Your artifacts are available at {run.info.artifact_uri}\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}